{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-9HWHF3iPm0"
      },
      "source": [
        "## Data collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W7IXMisLiPm0"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/sayakpaul/Action-Recognition-in-TensorFlow/releases/download/v1.0.0/ucf101_top5.tar.gz\n",
        "!tar -xf ucf101_top5.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1WDkB9UiPm0"
      },
      "source": [
        "## Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "i4B6mcZOiPm0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\n",
        "from tensorflow.keras.activations import gelu, softmax\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import History\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2 as cv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMYCbIsiiPm0"
      },
      "source": [
        "## Define Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KZjK6ZUxiPm0"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReX1a-cjiPm0"
      },
      "source": [
        "## Preparing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Up The Data Frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO]  Total videos for training: 594\n",
            "[INFO]  Total videos for testing: 224\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "print(f'[INFO]  Total videos for training: {len(train_df)}')\n",
        "print(f'[INFO]  Total videos for testing: {len(test_df)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame: np.ndarray) -> tf.Tensor:\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = tf.convert_to_tensor(cropped)\n",
        "    cropped = tf.squeeze(cropped)\n",
        "    return cropped\n",
        "\n",
        "\n",
        "def load_video(path: str, max_frames: int = 0) -> np.ndarray:\n",
        "    cap = cv.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            success, frame = cap.read()\n",
        "            if not success:\n",
        "                break\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frame = crop_center(frame)\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "def build_feature_extractor() -> keras.Model:\n",
        "    base_net = DenseNet121(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        pooling='avg',\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = base_net(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name='feature_extractor')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_extractor = build_feature_extractor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['CricketShot', 'PlayingCello', 'Punch', 'ShavingBeard', 'TennisSwing']\n"
          ]
        }
      ],
      "source": [
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df['tag']), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df: pd.DataFrame, root_dir: str) -> tuple[np.ndarray, np.ndarray]:\n",
        "    num_samples = len(df)\n",
        "    video_paths = df['video_name'].values.tolist()\n",
        "    labels = df['tag'].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=np.float32\n",
        "    )\n",
        "\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        temp_frame_features_placeholder = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype='float32'\n",
        "        )\n",
        "\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features_placeholder[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features_placeholder[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features_placeholder.squeeze()\n",
        "\n",
        "    return frame_features, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n",
        "!!tar -xf top5_data_prepared.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO]   Frame features int train set: (594, 20, 1024)\n"
          ]
        }
      ],
      "source": [
        "train_data, train_labels = np.load('train_data.npy'), np.load('train_labels.npy')\n",
        "test_data, test_labels = np.load('test_data.npy'), np.load('test_labels.npy')\n",
        "\n",
        "print(f'[INFO]   Frame features in train set: {train_data.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build The Transformer-Based Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Build A Custom Layer To Create Positional Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "h50L6NLwiPm1"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.position_embeddings.build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = tf.cast(x, self.compute_dtype)\n",
        "        length = tf.shape(x)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return x + embedded_positions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op0tEqMsiPm1"
      },
      "source": [
        "#### Build An Encoder Layer Of The Transformer Architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "rUTJBjEjiPm1"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "\n",
        "        self.norm_1 = layers.LayerNormalization()\n",
        "        self.norm2 = layers.LayerNormalization()\n",
        "\n",
        "        self.dense1 = layers.Dense(dense_dim, activation=gelu)\n",
        "        self.dense2 = layers.Dense(embed_dim)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        attention_output = self.attention(x, x, attention_mask=mask)\n",
        "        dense1_input = self.norm_1(x + attention_output)\n",
        "        dens1_output = self.dense1(dense1_input)\n",
        "        dens2_output = self.dense2(dens1_output)\n",
        "        return self.norm2(dense1_input + dens2_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKSDTQsLiPm2"
      },
      "source": [
        "#### Utility Functions For Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xv1_cManiPm2"
      },
      "outputs": [],
      "source": [
        "def get_compiled_model(shape: tuple) -> keras.Model:\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "\n",
        "    inputs = keras.Input(shape=shape)\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name='frame_position_embedding'\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name='transformer_layer')(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(.5)(x)\n",
        "    outputs = layers.Dense(classes, activation=softmax)(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(),\n",
        "        loss=sparse_categorical_crossentropy,\n",
        "        metrics=['accuracy'],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_experiment() -> keras.Model:\n",
        "    filepath = '/tmp/video_classifier.weights.h5'\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = get_compiled_model(train_data.shape[1:])\n",
        "    history: History = model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_split=0.15,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(filepath)\n",
        "    accuracy = model.evaluate(test_data, test_labels)[1]\n",
        "    print(f'Test accuracy: {round(accuracy * 100, 2)}%')\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVUp7IYFiPm2"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqiqHYI-iPm2",
        "outputId": "502f7f09-ec99-4966-d55d-d13caebf1aaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.9975 - accuracy: 0.6187\n",
            "Epoch 1: val_loss improved from inf to 2.16958, saving model to /tmp/video_classifier.weights.h5\n",
            "16/16 [==============================] - 8s 57ms/step - loss: 1.9238 - accuracy: 0.6290 - val_loss: 2.1696 - val_accuracy: 0.3667\n",
            "Epoch 2/5\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.1987 - accuracy: 0.9351\n",
            "Epoch 2: val_loss did not improve from 2.16958\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1679 - accuracy: 0.9464 - val_loss: 4.1225 - val_accuracy: 0.3333\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9861\n",
            "Epoch 3: val_loss did not improve from 2.16958\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0258 - accuracy: 0.9861 - val_loss: 3.2273 - val_accuracy: 0.4556\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9921\n",
            "Epoch 4: val_loss improved from 2.16958 to 1.79922, saving model to /tmp/video_classifier.weights.h5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0316 - accuracy: 0.9921 - val_loss: 1.7992 - val_accuracy: 0.7333\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9960\n",
            "Epoch 5: val_loss did not improve from 1.79922\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0091 - accuracy: 0.9960 - val_loss: 3.8259 - val_accuracy: 0.4667\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.6704 - accuracy: 0.8973\n",
            "Test accuracy: 89.73%\n"
          ]
        }
      ],
      "source": [
        "trained_model = run_experiment()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fiXaZzl_nikf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test video path: v_ShavingBeard_g03_c02.avi\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 20s 20s/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n",
            " 1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 557ms/step\n",
            "  ShavingBeard: 100.00%\n",
            "  Punch:  0.00%\n",
            "  CricketShot:  0.00%\n",
            "  TennisSwing:  0.00%\n",
            "  PlayingCello:  0.00%\n"
          ]
        }
      ],
      "source": [
        "def load_video() -> np.ndarray:\n",
        "    return np.random.rand(30, IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "\n",
        "base_net = keras.applications.DenseNet121(\n",
        "    include_top=False, pooling='avg', input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "\n",
        "def prepare_single_video(frames) -> np.ndarray:\n",
        "    frame_features = np.zeros(\n",
        "        shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype='float32')\n",
        "\n",
        "    if len(frames) < MAX_SEQ_LENGTH:\n",
        "        diff = MAX_SEQ_LENGTH - len(frames)\n",
        "        padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "        frames = np.concatenate([frames, padding], axis=0)\n",
        "\n",
        "    frames = frames[None, ...]\n",
        "\n",
        "    for i, batch in enumerate(frames):\n",
        "        video_length = batch.shape[0]\n",
        "        length = min(MAX_SEQ_LENGTH, video_length)\n",
        "        for j in range(length):\n",
        "            if np.mean(batch[j, :]) > 0.0:\n",
        "                frame_features[i, j, :] = base_net.predict(batch[None, j, :])\n",
        "            else:\n",
        "                frame_features[i, j, :] = 0.0\n",
        "\n",
        "    return frame_features\n",
        "\n",
        "\n",
        "def predict_action(path):\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frames = load_video(os.path.join('test', path), offload_to_cpu=True)\n",
        "    frame_features = prepare_single_video(frames)\n",
        "    probabilities = trained_model.predict(frame_features)[0]\n",
        "\n",
        "    for i in np.argsort(probabilities)[::-1]:\n",
        "        print(f'  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%')\n",
        "\n",
        "    return frames\n",
        "\n",
        "\n",
        "def to_gif(images):\n",
        "    converted_images = images.astype(np.uint8)\n",
        "    imageio.mimsave('animation.gif', converted_images, fps=10)\n",
        "\n",
        "\n",
        "# Assuming test_df is a DataFrame with a column 'video_name' containing video file names\n",
        "test_video = np.random.choice(test_df['video_name'].values.tolist())\n",
        "print(f'Test video path: {test_video}')\n",
        "test_frames = predict_action(test_video)\n",
        "to_gif(test_frames[:MAX_SEQ_LENGTH])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UpOyZHIiPm2"
      },
      "source": [
        "The performance of our model is far from optimal, because it was trained on a\n",
        "small dataset."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "video_transformers",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
